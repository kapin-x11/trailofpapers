{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passing spatial data through a KDTree in Spark\n",
    "## Or: the best coffee shop in Hong Kong for Pokemon\n",
    "In my day job, I often work with spatial data. Inevitably when working with spatial data, you want to find the closest places to a given location. One of the most efficient ways to do this is with a KDTree, a type of binary search tree for space.\n",
    "\n",
    "My favourite analysis tool is Spark, but it took me a little while to figure out how to work with spatial data in Spark. Google doesn't yield any tutorials on using KDTrees in PySpark, so this is a short tutorial on how to do it. I thought it would be fun to use KDTrees to find out which coffee shops in Hong Kong have the most Pokemon nearby.\n",
    "\n",
    "Some shortcuts:\n",
    "\n",
    "[Parsing data using pandas](#load)\n",
    "\n",
    "[Using KDTrees in Spark](#kdtree)\n",
    "\n",
    "[Performance of DataFrames vs RDDs](#rdd)\n",
    "\n",
    "\n",
    "\n",
    "Before we get to the fun stuff, we need to import some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy.spatial import cKDTree\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load'></a>\n",
    "\n",
    "## Load pokemon sightings\n",
    "It was surprisingly hard to get pokemon data. I tried scraping the data myself, but Niantic (the developers of Pokemon Go) actively discourages that. In the end, I found the easiest place to get data was a Hong Kong Pokemon tracker, https://pokemon.appx.hk/. I visited that website ten times, and viewed the source code to get JSON objects with Pokemon locations (to get the locations, hit F12 to access the Source mode, then go to the Network tab, then the XHR sub-tab, and select the top element. If the `top` object is not there, try reloading the page with source view open).\n",
    "\n",
    "After I downloaded the Pokemon location JSONs, I made the helper function below to load them using pandas, then created a single dataframe with all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_poke_json( pokefile):\n",
    "    return pd.read_json( pokefile ).rename(columns={'a':'lat', 'i':'type', 't':'ts', 'o':'lon'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ia</th>\n",
       "      <th>id</th>\n",
       "      <th>is</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>m1</th>\n",
       "      <th>m2</th>\n",
       "      <th>p</th>\n",
       "      <th>ts</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10341.000000</td>\n",
       "      <td>10315.000000</td>\n",
       "      <td>10345.000000</td>\n",
       "      <td>16565.000000</td>\n",
       "      <td>16565.000000</td>\n",
       "      <td>11044.000000</td>\n",
       "      <td>11044.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1.656500e+04</td>\n",
       "      <td>16565.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.035587</td>\n",
       "      <td>7.931265</td>\n",
       "      <td>7.989270</td>\n",
       "      <td>22.359070</td>\n",
       "      <td>114.120641</td>\n",
       "      <td>221.798080</td>\n",
       "      <td>77.897320</td>\n",
       "      <td>41.047619</td>\n",
       "      <td>1.482486e+09</td>\n",
       "      <td>65.941020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.325612</td>\n",
       "      <td>4.305660</td>\n",
       "      <td>4.281449</td>\n",
       "      <td>0.079485</td>\n",
       "      <td>0.098141</td>\n",
       "      <td>12.207434</td>\n",
       "      <td>33.937579</td>\n",
       "      <td>38.433678</td>\n",
       "      <td>7.234948e+05</td>\n",
       "      <td>34.650488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.168887</td>\n",
       "      <td>113.843555</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.481390e+09</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>22.294242</td>\n",
       "      <td>114.038069</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.481742e+09</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>22.360883</td>\n",
       "      <td>114.135435</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.482811e+09</td>\n",
       "      <td>63.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>22.425355</td>\n",
       "      <td>114.194700</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.483210e+09</td>\n",
       "      <td>92.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>22.553828</td>\n",
       "      <td>114.378200</td>\n",
       "      <td>242.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>1.483315e+09</td>\n",
       "      <td>149.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ia            id            is           lat           lon  \\\n",
       "count  10341.000000  10315.000000  10345.000000  16565.000000  16565.000000   \n",
       "mean       8.035587      7.931265      7.989270     22.359070    114.120641   \n",
       "std        4.325612      4.305660      4.281449      0.079485      0.098141   \n",
       "min        1.000000      1.000000      1.000000     22.168887    113.843555   \n",
       "25%        4.000000      4.000000      4.000000     22.294242    114.038069   \n",
       "50%        8.000000      8.000000      8.000000     22.360883    114.135435   \n",
       "75%       12.000000     12.000000     12.000000     22.425355    114.194700   \n",
       "max       15.000000     15.000000     15.000000     22.553828    114.378200   \n",
       "\n",
       "                 m1            m2           p            ts          type  \n",
       "count  11044.000000  11044.000000   21.000000  1.656500e+04  16565.000000  \n",
       "mean     221.798080     77.897320   41.047619  1.482486e+09     65.941020  \n",
       "std       12.207434     33.937579   38.433678  7.234948e+05     34.650488  \n",
       "min      200.000000     13.000000   16.000000  1.481390e+09      2.000000  \n",
       "25%      210.000000     56.000000   16.000000  1.481742e+09     35.000000  \n",
       "50%      222.000000     84.000000   19.000000  1.482811e+09     63.000000  \n",
       "75%      234.000000    105.000000   41.000000  1.483210e+09     92.000000  \n",
       "max      242.000000    133.000000  129.000000  1.483315e+09    149.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hk_poke_json = !ls hk-*.json\n",
    "hk_poke_df = pd.concat(map(load_poke_json, hk_poke_json))\n",
    "hk_poke_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data from appx.hk has four main columns of interest: pokemon type, lat, lon, and timestamp. I'm not sure what the `ia`, `id`, and `is` columns are for, since they are often null, and have no correlation with the other columns. There are over 16,000 pokemon sightings. We can use `value_counts` to find out which pokemon are most common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35     2823\n",
       "79     1361\n",
       "102     866\n",
       "27      766\n",
       "42      758\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hk_poke_df.type.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(hk_poke_df.type.value_counts() < 10).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of pokemon is highly skewed, with two types of Pokemon making up >30% of the total. Around 20% of Pokemon are so rare, they seen < 10 times.\n",
    "\n",
    "Now that we have a single dataset, we can save it as a csv to load into PySpark. (You can actually load the data directly using `spark.create.DataFrame()`, but wanted to save the intermediate dataset so people don't have to remake it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hk_poke_df[['lat', 'lon', 'ts', 'type']].to_csv( 'hk_pokemon.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Hong Kong coffee locations\n",
    "To get coffee shop locations, I queried the [FourSquare API](https://developer.foursquare.com/), using this [python package](https://pypi.python.org/pypi/foursquare/). As a free plug, I've found the FourSquare API has the most user friendly API of any review website. I adapted a [scraper function from another project](https://github.com/map222/Kibbeh/blob/master/src/API_io.py#L75) to get a list of Hong Kong coffee shops by lat / long.\n",
    "\n",
    "I used the scraper to create a list of 586 coffee shops, and saved it into `hk_cofee.json`. (There are probably a lot more coffee shops, but refining the coffee shop scraper is beyond the scope of this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(586, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hk_coffee_df = pd.io.json.json_normalize( json.loads(open('hk_coffee.json').read() ) )[['location.lat', 'location.lng', 'name']]\n",
    "hk_coffee_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the locations of the coffee shops to make sure we have good coverage of the island. In the end, the easiest way for me to plot this was to use [this handy website.](http://www.darrinward.com/lat-long/) ![Hong Kong Coffee Shops](https://raw.githubusercontent.com/map222/trailofpapers/master/pokemon/HK%20Coffee.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the locations are loaded, we can turn them into a scipy KDTree. Scipy has two types of KDTree in the library, compiled in python or C. I strongly recommend using the C version (cKDTree), as it is orders of magnitude quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coffee_kdtree = cKDTree(hk_coffee_df[['location.lat', 'location.lng']] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kdtree'></a>\n",
    "# Querying KDTrees in Spark\n",
    "Now that we have our Pokemon and coffee shop data processed, we can load the data into Spark! First, we can start by broadcasting the coffee_kdtree to all of the executors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark import Row\n",
    "coffee_tree_broadcast = sc.broadcast( coffee_kdtree )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to load the pokemon data into Spark. Here I define a simple schema using StructType, then load the pokemon csv using `spark.read.csv`, and finally `persist` the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poke_schema = T.StructType([\n",
    "                T.StructField('lat', T.FloatType(), False),\n",
    "                T.StructField('lon', T.FloatType(), False),\n",
    "                T.StructField('ts', T.FloatType(), False),\n",
    "                T.StructField('type', T.IntegerType(), False)\n",
    "                ])\n",
    "poke_df = spark.read.csv('hk_pokemon.csv', schema = poke_schema).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to make a function to query the KDTree. The function takes two columns from the DataFrame, and passes them through the KDTree one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_kdtree( lat, lon, cur_tree, radius = 0.0005):\n",
    "    ''' query the kdtree to find locations within a radius of the lat / lon\n",
    "        lat / lon: native python floats representing location\n",
    "        cur_tree: a broadcast scipy cKDTree\n",
    "        radius: radius, in degrees, around the pokemon to accept pokestops (default to ~50m)\n",
    "    '''\n",
    "    \n",
    "    # a standard scipy cKDTree query, we want up to 10 coffeehouses within the radius of each pokemon\n",
    "    query_results = cur_tree.value.query([lat, lon], 10, distance_upper_bound=radius)\n",
    "    \n",
    "    # query_results is a list of two arrays (radius, and index); we want to return the index\n",
    "    # the index is a numpy integer which we needs to be cast as a python integer to return to Spark\n",
    "    return [int(row[1]) for row in zip(*query_results) if row[0] < radius]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working in PySpark, we need to wrap the above function in a udf wrapper (user-defined function). The udf will be passed two columns, but we also need to set the KDTree as an argument. Here I use the `partial` function to pass `coffee_tree_broadcast`. You should also be able to use a `lambda` function to similar effect. Udf's require you to specify the return type. In this case, we are returning a list of integers, which are indices in the KDTree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "coffee_udf = F.udf( partial(query_kdtree, cur_tree = coffee_tree_broadcast),\n",
    "                    T.ArrayType( T.IntegerType() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to pass the pokemon through the KDTree! To do so, we just need to add a column to the DataFrame using `withColumn`, applying the `coffee_udf`. Note that we can pass the lat and lon columns simply by naming them. After creating the new column, I use an `explode` statement to separate each shop into its own row. This should make the dataset tidier\\*, as each row will represent one pokemon-shop combination.  Notice that after the explosion, the bottom two rows represent a single pokemon, and the two coffee shops it was near.\n",
    "\n",
    "\\*I think that's what tidy means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------+----+------------+----------+\n",
      "|      lat|      lon|          ts|type|nearby_shops|shop_index|\n",
      "+---------+---------+------------+----+------------+----------+\n",
      "|22.308178|113.91828|1.48331162E9|  27|         [0]|         0|\n",
      "|22.207895|114.02905|1.48331187E9|  17|        [40]|        40|\n",
      "|22.320929|113.94459|1.48331226E9|  56|        [17]|        17|\n",
      "|22.205046|114.02998|1.48331162E9|  74|        [38]|        38|\n",
      "|22.338083|114.13906|1.48331149E9|  35|    [92, 96]|        92|\n",
      "|22.338083|114.13906|1.48331149E9|  35|    [92, 96]|        96|\n",
      "+---------+---------+------------+----+------------+----------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nearby_df = (poke_df.withColumn('nearby_shops', coffee_udf('lat', 'lon'))\n",
    "                    .select('*', F.explode('nearby_shops').alias('shop_index') ).persist() )\n",
    "nearby_df.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data we can answer some simple questions. For example, how many pokemon had a coffee shop nearby?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearby_df.select('lat', 'lon', 'ts', 'type').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can ask how many shops had at least one Pokemon nearby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(nearby_df.select('shop_index').distinct().count() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can answer the question, which are the best coffee shops to catch pokemon? To do that, we can groupby store_index, and then aggregate by count. For display, we want to sort using `orderBy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|shop_index|num_pokemon|\n",
      "+----------+-----------+\n",
      "|        40|          6|\n",
      "|        64|          5|\n",
      "|       333|          5|\n",
      "|       295|          5|\n",
      "|       168|          5|\n",
      "+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(nearby_df.groupby('shop_index')\n",
    "          .agg(F.count('shop_index').alias('num_pokemon'))\n",
    "          .orderBy('num_pokemon', ascending = False).show(5) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see which shops these are, we can go back to the original coffeshop dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location.lat</th>\n",
       "      <th>location.lng</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>22.207968</td>\n",
       "      <td>114.029331</td>\n",
       "      <td>Coffee Seeds 阿翁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>22.270550</td>\n",
       "      <td>114.130811</td>\n",
       "      <td>Pacific Coffee (太平洋咖啡)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>22.318484</td>\n",
       "      <td>114.174676</td>\n",
       "      <td>Starbucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>22.296741</td>\n",
       "      <td>114.169362</td>\n",
       "      <td>Starbucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>22.283222</td>\n",
       "      <td>114.159201</td>\n",
       "      <td>Starbucks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     location.lat  location.lng                    name\n",
       "40      22.207968    114.029331         Coffee Seeds 阿翁\n",
       "64      22.270550    114.130811  Pacific Coffee (太平洋咖啡)\n",
       "333     22.318484    114.174676               Starbucks\n",
       "295     22.296741    114.169362               Starbucks\n",
       "168     22.283222    114.159201               Starbucks"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hk_coffee_df.loc[[40, 64, 333, 295, 168]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out the best place is a [local place on a tiny southern island](https://foursquare.com/v/coffee-seeds-%E9%98%BF%E7%BF%81/4f547dbfe4b0bf6b5f624b57/)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hk_coffee_df.name.str.startswith('Starbucks').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For kicks, we can also see how many were Starbucks. 150 out of 580! This probably reflects the limitations of our original dataset, as we scraped the coffee shop locations using Four Square. If anyone has a better coffee shop list, let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rdd'></a>\n",
    "# Performance of DataFrames vs RDDs\n",
    "If you go to any Spark workshop or meetup, you will inevitably be told that DataFrames are a lot faster and easier to work with than RDDs. While this dataset is a probably too small for a proper investigation, I thought I'd run some benchmarks to see. To start with, let's see how fast the query is using DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 1.68 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16565"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time (poke_df.repartition(1).withColumn('nearby_shops', coffee_udf('lat', 'lon')).count() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.7 seconds for our 16,000 locations using DataFrames. For RDDs, we can write a similar function, and `map` it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_kdtree_rdd( row, map_tree, radius = 0.0005):\n",
    "    # first we need to convert the row to a dictionary\n",
    "    cur_row = row.asDict()\n",
    "    \n",
    "    # pass the dictionary through the KDTree\n",
    "    query_results = map_tree.value.query([row['lat'], row['lon']], 10, distance_upper_bound=radius)\n",
    "    \n",
    "    # then convert the results back into a dictionary / row\n",
    "    cur_row['indices'] =  [int(row[1]) for row in zip(*query_results) if row[0] < radius]\n",
    "    return Row(**cur_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 4 ms, total: 12 ms\n",
      "Wall time: 1.62 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16565"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time poke_df.rdd.map(partial(query_kdtree_rdd, map_tree = coffee_tree_broadcast)).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a little bit slower! What if we use `mapPartitions`, which processes an entire partition in one go?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# naive mapPartitions\n",
    "def query_kdtree_mappartitions(iterator):\n",
    "    radius = 0.0005\n",
    "    new_rows = []\n",
    "    for row in iterator:\n",
    "        cur_row = row.asDict()\n",
    "        query_results = coffee_tree_broadcast.value.query([cur_row['lat'], cur_row['lon']], 10, radius)\n",
    "        cur_row['indices'] =  [int(row[1]) for row in zip(*query_results) if row[0] < radius]\n",
    "        new_rows.append(Row(**cur_row))\n",
    "    return new_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 1.09 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16565"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time poke_df.rdd.mapPartitions(query_kdtree_mappartitions).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30% faster! In previous KDTree benchmarks I've tried, the DataFrame method is ~2x faster than the mapPartitions method. My guess is that this dataset is small enough that the advantages of DataFrames can't shine through. \n",
    "\n",
    "\n",
    "## Conclusion\n",
    "Anyway, this is one way to use KDTrees in PySpark! Some of the key general concepts for me were:\n",
    "1. Learning how to define `UDF`s on multiple columns of data\n",
    "2. Figuring out how to pass the KDTree to the function using `partial`\n",
    "3. Being careful to return native python types, and using the right DataTypes\n",
    "    \n",
    "Good Pokehunting and tree searching!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
