{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why are some athletes more popular than others?\n",
    "## Part II: Calculating sentiment towards players\n",
    "In this project, I am using natural language processing (NLP) to try to understand what factors drive public opinion towards athletes. In part 1, I showed how to use the pushShift API to scrape comment and voting data off of the r/NBA and r/NFL sub-reddits and join it to other auxiliary data on those athletes. In this notebook, I will cover how I identified which players each comment was about; and how to calculate sentiments towards a player. In part 3, I will fit regression models to comment sentiment and votes in order to determine which features of an athlete are predictive of sentiment.\n",
    "\n",
    "## What are entities and sentiment?\n",
    "In this notebook I am going to use a lot of jargon from NLP. Before diving into how I calculated opinion towards players, let's review a few terms:\n",
    "* Corpus: A corpus is a collection of documents\n",
    "* Token: A token is a single word in a sentence (some models split single words into multiple tokens, or include separators as tokens)\n",
    "* Named entity: In NLP, an \"entity\" is a noun. Thus a *named* entity is just a proper noun. The most common named entity in basketball right now is \"LeBron.\"\n",
    "* Named entity recognition (NER): The task of identifying which tokens in a sentence are named entities. A simple NER model would use things like capitalization to identify named entities. For example, \"Nice assist by Wall,\" would identify the player John Wall rather than the wall of a building. More complex NER models use part-of-speech tagging or even neural nets to identify named entities.\n",
    "* Sentiment analysis: The task of identifying whether a sentence or document is generally positive or negative. Simple models assign a positive or negative value to each word (e.g. \"love\" is a positive word). More complex models assign sentiment for each entity in a document. entiment models are typically trained to be effective for a specific task.\n",
    "\n",
    "## How do we calculate the sentiment towards a player?\n",
    "The data I scraped in part 1 constitutes a corpus of comments about NBA and NFL players. These comments range from short exclamations about specific players (\"Cedi is the GOAT!\"), to longer comments involving multiple named entities (\"JR Smith threw a bowl of chicken tortilla soup at Damon Jones.\").\n",
    "\n",
    "Probably the most accurate way to calculate sentiment towards players would be to use a combined entity-sentiment model. These models parse each sentence for parts of speech and named entities, and assign sentiment towards each named entity. For example, a combined model could take \"LeBron is better than Jordan,\" and assign positive sentiment to LeBron directly. For purposes of simplicity and computational ease, I did not take this approach but it may be useful to explore in future work.\n",
    "\n",
    "Instead, I restricted my focus to sentences that contained only a single named entity; then, for each such sentence, I calculated the overall sentiment of that sentence and linked it to that single entity. The downsides to this approach are: (1) I had to throw away information from sentences that contained multiple named entities; and (2) I had to assume the overall sentiment of a sentence reflects sentiment towards the player in the sentence. Fortunately, I had more than enough data to overcome the first obstacle. As for the second, we will have to assume that average sentiment in a player's single-entity sentences is a useful proxy for overall sentiment direct toward that player."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying which athletes a sentence is about\n",
    "I took two broad approaches to named entity recognition. First, I tried using [Stanford's NER](https://nlp.stanford.edu/software/CRF-NER.shtml) package. My other approach was to use a known list of named entities (viz. NBA and NFL players' names), and then simply check to see if these names were present. In this section, I am going to demonstate both approaches on a small corpus. \n",
    "\n",
    "A re-usable version of  this process is in the function [`create_sentiment_df`](https://github.com/map222/trailofpapers/blob/sentiment_sports/sentiment_sports/sports_sentiment.py#L12) in the module [`sentiment_sports.py`](https://github.com/map222/trailofpapers/blob/sentiment_sports/sentiment_sports/sports_sentiment.py).\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import sent_tokenize\n",
    "import string\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from sner import Ner\n",
    "from nltk import sent_tokenize\n",
    "from fuzzywuzzy import process as fuzzy_process\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking comments into sentences\n",
    "The output of the Reddit scraping was full comments, which could contain multiple sentences. To get more samples, we can chunk the comments into sentences. I wrote the helper function [`chunk_comments_sentences`](https://github.com/map222/trailofpapers/blob/sentiment_sports/sentiment_sports/sports_sentiment.py#L72) to do just this. An example of this `pandas` processing is in the Appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity recognition\n",
    "Now that we have sentences, we can do NER two ways. First, we can do naive NER, where we don't know the entities ahead of time. Or we can do known NER, where we know what entities we care about\n",
    "#### Using Stanford's NER tagger\n",
    "Stanford's NLP department has provided a whole suite of tools, including an NER Tagger. Running this tagger in python is quite slow, so I used a package called [`sner` from caihaoyu](https://github.com/caihaoyu/sner), which runs a Java server locally that you can query. To get this running, there are four steps:\n",
    "1. Install sner (`pip install sner` worked for me, but you can see the package for details\n",
    "2. Download the [Stanford NER package](https://nlp.stanford.edu/software/CRF-NER.shtml#Download), and unzip it in your preferred location. I usually do it under the `sner` package directory, e.g. `C:\\Users\\map22\\Anaconda3\\Lib\\site-packages\\sner\\stanford-ner-2018-10-16`\n",
    "3. Open a command prompt and navigate to that directory\n",
    "4. Run the java server:\n",
    "\n",
    "  `java -Djava.ext.dirs=./lib -cp stanford-ner.jar edu.stanford.nlp.ie.NERServer -port 9199 -loadClassifier ./classifiers/english.all.3class.distsim.crf.ser.gz  -tokenizerFactory edu.stanford.nlp.process.WhitespaceTokenizer -tokenizerOptions tokenizeNLs=false`\n",
    "\n",
    "Once you have all that done, you can start tagging entities! For dataframes, I created the function [`extract_unknown_ner`](https://github.com/map222/trailofpapers/blob/sentiment_sports/sentiment_sports/sports_sentiment.py#L95). Here is an example of tagging a single sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Isaiah', 'PERSON'),\n",
       " ('Thoms', 'PERSON'),\n",
       " ('is', 'O'),\n",
       " ('the', 'O'),\n",
       " ('worst.', 'O')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagger = Ner(host='localhost',port=9199)\n",
    "pos_tagger.get_entities('Isaiah Thomas is the worst.' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the NER tagger are tuples of (token, entity type), where entity type can be `PERSON`, `ORG`, or `O` for no entity. Here, both Isaiah and Thomas have been tagged as `PERSON` entities. What happens when we try to tag Cedi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Cedi', 'O'), ('is', 'O'), ('the', 'O'), ('GOAT!', 'O')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagger.get_entities('Cedi is the GOAT!' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the tagger failed to tag Cedi. This is one of the major problems using off-the-shelf NER tools: they are trained and tuned to general applications, which may not transfer to the context you care about. The other major problem is that this tagger is sloooow. Even using a java server, it can take hours to extract entities from one year of data. This makes iterating and finding mistakes hard.\n",
    "\n",
    "### NER using a known list\n",
    "NER taggers are most useful in cases where you're not sure what the entities are, or you might find new named entities. Neither of those situations apply to us: we only care about NBA and NFL-related entities, and we already know those ahead of time!\n",
    "\n",
    "Instead, I used the following algorithm to identify named entities in a sentence:\n",
    "1. Get a set of all of the named entities you care about. For us, these are mainly player, team, and coach names.\n",
    "2. For all names that match common English words (e.g. \"Wall\" or \"Love\"), find capitalized tokens. For example, this would recognize \"assist by John Wall,\" but not \"Tatum hit the rookie wall.\"\n",
    "3. For all other names, just find a direct match. Most names are uncommon enough that they only match exact players.\n",
    "\n",
    "For my dataframe processing, I wrote the function [`extract_known_ner`](https://github.com/map222/trailofpapers/blob/sentiment_sports/sentiment_sports/sports_sentiment.py#L122). Here's what this would look like for a toy dataset. First, we can create a small set of known entities, one of which we only want to recognize when capitalized (\"Wall\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love Cedi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kevin Love is the best player on the Cavs now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LeBron told Love to fit in, not fit out</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         comment\n",
       "0                                    I love Cedi\n",
       "1  Kevin Love is the best player on the Cavs now\n",
       "2        LeBron told Love to fit in, not fit out"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a set for fast lookup\n",
    "UPPER_SET = {'Love', }\n",
    "LOWER_SET = {'cedi', 'kevin', 'cavs', 'lebron'}\n",
    "toy_df = pd.DataFrame({'comment':['I love Cedi', 'Kevin Love is the best player on the Cavs now',\n",
    "                                  'LeBron told Love to fit in, not fit out']})\n",
    "toy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can extract the upper case matches. Notice that it doesn't match \"love Cedi.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>upper_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love Cedi</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kevin Love is the best player on the Cavs now</td>\n",
       "      <td>[Love]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LeBron told Love to fit in, not fit out</td>\n",
       "      <td>[Love]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         comment upper_entities\n",
       "0                                    I love Cedi             []\n",
       "1  Kevin Love is the best player on the Cavs now         [Love]\n",
       "2        LeBron told Love to fit in, not fit out         [Love]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT_COL = 'comment'\n",
    "upper_filter = lambda sentence: [word for word in sentence.split() if word in UPPER_SET]\n",
    "toy_df['upper_entities'] = toy_df[TEXT_COL].apply(upper_filter)\n",
    "toy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can extract the lower case entities and combine them with the upper case entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>upper_entities</th>\n",
       "      <th>lower_entities</th>\n",
       "      <th>combined_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i love cedi</td>\n",
       "      <td>[]</td>\n",
       "      <td>[cedi]</td>\n",
       "      <td>[cedi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kevin love is the best player on the cavs now</td>\n",
       "      <td>[Love]</td>\n",
       "      <td>[kevin, cavs]</td>\n",
       "      <td>[kevin, cavs, love]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lebron told love to fit in, not fit out</td>\n",
       "      <td>[Love]</td>\n",
       "      <td>[lebron]</td>\n",
       "      <td>[lebron, love]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         comment upper_entities  \\\n",
       "0                                    i love cedi             []   \n",
       "1  kevin love is the best player on the cavs now         [Love]   \n",
       "2        lebron told love to fit in, not fit out         [Love]   \n",
       "\n",
       "  lower_entities    combined_entities  \n",
       "0         [cedi]               [cedi]  \n",
       "1  [kevin, cavs]  [kevin, cavs, love]  \n",
       "2       [lebron]       [lebron, love]  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_df[TEXT_COL] = toy_df[TEXT_COL].str.lower()\n",
    "\n",
    "# tokenize sentence with split, and use filter to find named entities\n",
    "ner_filter = lambda sentence: [word for word in sentence.split() if word in LOWER_SET]\n",
    "toy_df['lower_entities'] = toy_df[TEXT_COL].apply(ner_filter)\n",
    "toy_df['combined_entities'] = toy_df.apply(lambda row: row['lower_entities'] + [word.lower() for word in row['upper_entities']], axis=1)\n",
    "toy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying comments about one player\n",
    "#### Filtering out comments with too many named entities\n",
    "After extracting the named entities, we next have to tie them to a known player. I do this in two steps.\n",
    "\n",
    "First, I filter out all sentences that don't contain any entities, and those sentences with more than two entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>upper_entities</th>\n",
       "      <th>lower_entities</th>\n",
       "      <th>combined_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i love cedi</td>\n",
       "      <td>[]</td>\n",
       "      <td>[cedi]</td>\n",
       "      <td>[cedi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lebron told love to fit in, not fit out</td>\n",
       "      <td>[Love]</td>\n",
       "      <td>[lebron]</td>\n",
       "      <td>[lebron, love]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   comment upper_entities lower_entities  \\\n",
       "0                              i love cedi             []         [cedi]   \n",
       "2  lebron told love to fit in, not fit out         [Love]       [lebron]   \n",
       "\n",
       "  combined_entities  \n",
       "0            [cedi]  \n",
       "2    [lebron, love]  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_df = toy_df[toy_df['combined_entities'].str.len() > 0] # only care if we can find entity\n",
    "toy_df = toy_df[toy_df['combined_entities'].str.len() < 3]\n",
    "toy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this for dataframes, I wrote the functionn [`clean_entities`](https://github.com/map222/trailofpapers/blob/sentiment_sports/sentiment_sports/sports_sentiment.py#L145). \n",
    "\n",
    "#### Fuzzy matching the entities to players\n",
    "At this point, we have sentences with 1-2 entities, and we have to figure out whether these entities match known players. For example, the single entity \"Cedi\" obviously refers to Cedi Osman, while a single entity like \"Kevin\" could mean Kevin Durant or Kevin Love. For sentences with two entities, we need to figure out whether they are referring to the same player (\"LeBron James\") or different players (\"LeBron, Love\").\n",
    "\n",
    "To perform this task, I used fuzzy matching, which calculates the edit distance between two strings. It does this by looking for token matches, and substring matches. For this process, I used the package [`fuzzywuzzy`](https://github.com/seatgeek/fuzzywuzzy). The below function matches a potential player name against a list of known player names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWN_PLAYERS = {'lebron james', 'cedi osman', 'kevin love'}\n",
    "def find_player( potential_name: str, unique_names: set ):\n",
    "    ''' Fuzzy match an entity to one entity from a set\n",
    "\n",
    "        potential_name: string name of an entity (e.g. a player name, like \"lebron\")\n",
    "        unique_names: set of string player names ('first last') to be matched against\n",
    "    '''\n",
    "    names = fuzzy_process.extractBests(potential_name, unique_names, score_cutoff=87) # 87 means that \"klay kd\" matches nothing, but \"curry\" matches \"stephen curry\"\n",
    "    \n",
    "    # if there are more than 2 names, and they have the same score, no clear match\n",
    "    if (len(names) > 1 and names[0][1] == names[1][1]) or len(names) == 0:\n",
    "    # no clear match, return 'unclear match'\n",
    "        return 'unclear'\n",
    "    # there is only one similar name, or a clear top similar name\n",
    "    return names[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the fuzzy matching, I first convert the list of entities into a single string, then apply the `find_player` function. If there is a clear match, the name is returned. If the match is unclear, \"unclear\" is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>combined_entities</th>\n",
       "      <th>fuzzy_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i love cedi</td>\n",
       "      <td>[cedi]</td>\n",
       "      <td>cedi osman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lebron told love to fit in, not fit out</td>\n",
       "      <td>[lebron, love]</td>\n",
       "      <td>unclear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   comment combined_entities fuzzy_match\n",
       "0                              i love cedi            [cedi]  cedi osman\n",
       "2  lebron told love to fit in, not fit out    [lebron, love]     unclear"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_df['str_entities'] = toy_df['combined_entities'].str.join(' ')\n",
    "toy_df['fuzzy_match'] = toy_df['str_entities'].apply(lambda names: find_player(names, KNOWN_PLAYERS))\n",
    "toy_df[['comment', 'combined_entities', 'fuzzy_match']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then filter out sentences with an unclear name match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df = toy_df.query('fuzzy_match != \"unclear\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speeding up fuzzy matching with dask\n",
    "The fuzzy matching is one of the most time consuming parts of the ETL pipeline, as it is basically O(N^2) (N sentences * M players). To speed it up I used [`dask`](http://docs.dask.org/en/latest/why.html) to parallelize my dataframe operations. For details, see the appendix.\n",
    "## Sentiment model\n",
    "At this point, we now have a dataframe of sentences, each of which we have assigned to a single NBA or NFL player. Now we need to figure out whether the comments are positive or not!\n",
    "\n",
    "As discussed above, I will use sentence-level sentiment analysis, and assume that the sentence level sentiment reflects the sentiment towards the player in the sentence. To calculate sentiment, I used probably the most famous sentiment analyzer out there, [VADER](http://www.nltk.org/howto/sentiment.html). VADER uses a pre-defined dictionary of sentiment for most common words in English (out-of-vocabulary words are given neutral sentiment). For example, here the sentiment for the word \"steal\" is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.2"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "sid.lexicon['steal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the overall sentiment for a sentence, VADER simply sums up the sentiment of individual words.\n",
    "#### Updating the dictionary for sports\n",
    "The sentiment for many sports related words is quite different from general usage. For example, \"steal\" is generally a negative word, but in the context of basketball it is neutral. Luckily, it is relatively straightforward to update the lexicon, as the lexicon is just a python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_words = ['steal', 'steals', 'block', 'blocked', 'blocks', 'slam',\n",
    "              'charges', 'rejection', 'free', 'assists', 'win']\n",
    "sid.lexicon.update({word:0.0 for word in stat_words})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the sentiment analyzer\n",
    "This is as simple as calling the `polarity_scores` function on a sentence (and doing some pandas manipulation of the results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>upper_entities</th>\n",
       "      <th>lower_entities</th>\n",
       "      <th>combined_entities</th>\n",
       "      <th>str_entities</th>\n",
       "      <th>compound</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i love cedi</td>\n",
       "      <td>[]</td>\n",
       "      <td>[cedi]</td>\n",
       "      <td>[cedi]</td>\n",
       "      <td>cedi</td>\n",
       "      <td>0.6369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       comment upper_entities lower_entities combined_entities str_entities  \\\n",
       "0  i love cedi             []         [cedi]            [cedi]         cedi   \n",
       "\n",
       "   compound  neg    neu    pos  \n",
       "0    0.6369  0.0  0.192  0.808  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_df = toy_df[TEXT_COL].apply( sid.polarity_scores )\n",
    "sentiment_df = pd.DataFrame.from_dict(intermediate_df.tolist())\n",
    "toy_df = toy_df.join(sentiment_df)\n",
    "toy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analyzer returns four values (in alphabetical order):\n",
    "* compound: the overall sentiment for the sentence, combining positive, negative, and neutral\n",
    "* neg: score for number of negative words in the sentence\n",
    "* neu: score for neutral words\n",
    "* pos: score for positive words\n",
    "\n",
    "You can see in this instance there were only positive and neutral words, with the overall sentiment being positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "### Example of chunking comments into sentences\n",
    "First, let's start with a typical Cavs fan comment. I use `pandas` DataFrames for everything, so let's use one here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>flair</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cedi is the GOAT! Isaiah Thomas is the worst</td>\n",
       "      <td>CLE</td>\n",
       "      <td>map222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment flair    user\n",
       "0  Cedi is the GOAT! Isaiah Thomas is the worst   CLE  map222"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_df = pd.DataFrame({'comment':['Cedi is the GOAT! Isaiah Thomas is the worst'],\n",
    "                           'user': ['map222'], 'flair':'CLE'})\n",
    "comment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tokenize, I used NLTK's `sent_tokenize` function. Since multiple sentences can be returned from a comment, I did some manipulation to get back a Series with a row for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0  0             Cedi is the GOAT!\n",
       "   1    Isaiah Thomas is the worst\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df = (comment_df['comment'].apply(lambda row: pd.Series(sent_tokenize(row)))\n",
    "                                     .stack())\n",
    "sentences_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to do a bit more `pandas` manipulation to get a DataFrame where the index for each sentence is the same as its parent comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level_0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cedi is the GOAT!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Isaiah Thomas is the worst</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          sentences\n",
       "level_0                            \n",
       "0                 Cedi is the GOAT!\n",
       "0        Isaiah Thomas is the worst"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df = (sentences_df.reset_index()\n",
    "                  .set_index('level_0')\n",
    "                  .rename(columns={0:'sentences'})\n",
    "                  .drop(['level_1'], axis = 1))\n",
    "sentences_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the index is sorted out, we can rejoin the sentences to the original comments, which allows us to retain metadata like the user and flair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flair</th>\n",
       "      <th>user</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CLE</td>\n",
       "      <td>map222</td>\n",
       "      <td>Cedi is the GOAT!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CLE</td>\n",
       "      <td>map222</td>\n",
       "      <td>Isaiah Thomas is the worst</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  flair    user                   sentences\n",
       "0   CLE  map222           Cedi is the GOAT!\n",
       "0   CLE  map222  Isaiah Thomas is the worst"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_df = (comment_df.join(sentences_df)\n",
    "                        .drop(columns = ['comment']))\n",
    "comment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy matching using dask\n",
    "In my \"production\" code, I used `dask` to perform the fuzzy matching, in the function [`fuzzy_match_players`](https://github.com/map222/trailofpapers/blob/sentiment_sports/sentiment_sports/sports_sentiment.py#L189). Dask is a version of dataframes that allows for out-of-core computing for files that don't fit in memory; and more importantly for us, it allows for parallel processing. The way I use dask is relatively straightforward: I do all my work in pandas until I hit a particularly slow step. Then I convert my DataFrame from pandas to dask, do the slow step in parallel, then convert it back to pandas.\n",
    "\n",
    "Here is what the code looks like. Before using dask, we define the extent of our parallelism: the number of CPUs to use, and the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask as dd\n",
    "num_workers = 8\n",
    "num_partitions = int(2* num_workers - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since named entities are often duplicated, we extract the unique named entities, and create a single column dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_df = pd.DataFrame(sentiment_df[STR_COL].unique(), columns = [STR_COL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dask dataframe is as simple as calling `from_pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.from_pandas(fuzzy_df, npartitions=num_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask uses lazy execution. This means that it doesn't execute code unless you force it to. So to do our fuzzy matching, we need to define what we want dask to do using `map_partitions()`, then force execution using `compute()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf['fuzzy_name'] = ddf.map_partitions(lambda df: df['str_entities'].apply(lambda row: find_player(row, UNIQUE_NAMES) ) )\n",
    "fuzzy_df = ddf.compute(num_workers=num_workers, scheduler='processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, since we only got fuzzy matches for unique names, we need to join it back onto our original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = sentiment_df.merge(fuzzy_df, on=STR_COL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
