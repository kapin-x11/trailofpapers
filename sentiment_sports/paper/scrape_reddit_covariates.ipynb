{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why are some athletes more popular than others?\n",
    "The 2018 Cleveland Cavs made the NBA finals while generating some of the hottest memes of 2018 (\"We got an \\[expletive\\] squad now,\" \"He boomed me.\"). Following the Cavs on Reddit, I noticed something odd. Turkish rookie Cedi Osman was a particular fan favourite. Cedi played limited minutes with energy, and everyone joked that Cedi was the \"GOAT\" (Greatest Of All Time) carrying Lebron. In contrast, Tristan Thompson, a hero of the 2016 season, had an off year, and was the center of a meme for being traded, \"Shump, TT, and the Nets pick.\" For the Cavs at least, it seemed commenters gave the white players an easier time. And being a data scientist, I thought, \"I could measure that!\"\n",
    "\n",
    "In this project, I am going to show how I used sentiment and econometric models to understand what makes NBA and NFL players popular. I split this project into three parts. In this notebook, Part 1, I will focus on scraping player comments from Reddit, and how to download covariate data for players (e.g. performance and demographics). In [part 2](https://github.com/map222/trailofpapers/blob/sentiment_sports/sentiment_sports/paper/calculating_player_sentiment.ipynb), I will isolate comments about single players using named entity recognition, and show how to calculate sentiment towards players. In part 3, I will use econometric regression models to investigate what drives sentiment towards players."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we understand public opinion towards athletes?\n",
    "In the past, companies have quantified public opinion towards athletes using surveys like Q-score. While surveys provide precise quantitative answers, they are inherently biased, since only a small fraction of people respond to surveys. Furthermore, people's stated opinions may be different from their honest opinions, due to self-censoring, or lack of self-knowledge.\n",
    "\n",
    "Instead, thanks to social media and natural language processing techniques, we can analyze how people naturally talk about players. This allows us to get opinions from a wider group of people (although it still may overrepresent loud people), and to infer the public's real opinion. Social media also allows you to measure community effects as well, as people upvote and downvote opinions. The formation of opinion in these online communities is often a bellwether for how the general public will feel later.\n",
    "\n",
    "For the remainder of this notebook I am going to cover how I scraped Reddit for comments.\n",
    "## Part 1: Scraping data\n",
    "### Scraping comment data from Reddit\n",
    "My primary source of player comments were the active [NBA](https://www.reddit.com/r/nba) and [NFL](https://www.reddit.com/r/nfl) subreddits. I also tried scraping Twitter; while the Twitter API works well for live data, I found getting historical data difficult.\n",
    "\n",
    "There are a few different options for scraping data from Reddit. First, there is the [official Reddit API](https://www.reddit.com/dev/api/), which allows one to scrape reddit, post comments, and vote. I started this project using the official API, but stopped after an update broke my code (you can see this original code in the [appendix of this notebook](https://nbviewer.jupyter.org/github/map222/trailofpapers/blob/sentiment_sports/sentiment_sports/paper/calculating_player_sentiment.ipynb#appendix).\n",
    "\n",
    "For the majority of this project I used the [pushshift](pushshift.io) API. Pushshift is a third party social media database that scrapes reddit, and makes it available to others. I found this API more stable and easier to use since it did not require authentication.\n",
    "\n",
    "I originally did the reddit scraping in a Jupyter notebook before realizing that I was repeating myself. Taking a note from [Joel Grus](https://www.youtube.com/watch?v=7jiPeIFXb6U), I function-ified my notebook into a module [`scrape_reddit_comments`](https://nbviewer.jupyter.org/github/map222/trailofpapers/blob/sentiment_sports/sentiment_sports/paper/scrape_reddit_comments.py) to make it easier to use. In this section of the notebook I'm going to walk through the main parts of the function `get_month_pushshift` from that module, which scrapes submissions (original posts) and comments (replies) for one month of a subreddit.\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "data_path = 'd:/data/sentiment_sports/'\n",
    "import scrape_reddit_comments as src\n",
    "from scrape_reddit_comments import parse_submission_pushshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping code\n",
    "#### Initialize variables\n",
    "For this example, I want to scrape a few days of data from r/NBA for January 2018. Let's start by initializing variables for dates and times. The scraper works by stepping hour by hour through the month, so we can use datetime methods to calculate the start and end time of the month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "year, month, day = 2018, 1, 3\n",
    "start_after = datetime.now() - datetime(year,month,1)\n",
    "end_before = datetime.now() - datetime(year,month,day)\n",
    "start_hour = start_after.total_seconds()  // 3600\n",
    "end_hour = end_before.total_seconds()  // 3600 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the API query\n",
    "Pushshift uses a RESTful API, which means that it uses vanilla HTTP to serve data. For example, if you [click this link](https://api.pushshift.io/reddit/search/submission/?subreddit=nba&size=10), you should see a JSON object of 10 recent posts to /r/nba. Rather than using a browser and typing in URLs to get data, we can use the `requests` module to modify and process the URLs for us.\n",
    "\n",
    "To use `requests`, we need two pieces of information: the URL we are going to query; and parameters for the query like which subreddit we are interested in. In this example I am using the URL for original posts (\"submissions\"); and I set the parameter for the NBA subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_params = {'subreddit': 'nba',\n",
    "              'size':500}\n",
    "submission_url = 'https://api.pushshift.io/reddit/search/submission/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the query for a month\n",
    "Now that we have our basic query set up, we can start to get some data. Since the query is limited to 500 posts, to get get a whole month of data, we need to repeat the query for every day of the month, or even multiple times in a day (here we are only doing 6 days, as defined above). The `for` loop below does just that, modifying the time parameter with six hour increments. I also include a `time.sleep` to prevent the API from getting overloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading submissions for 2018-1\n"
     ]
    }
   ],
   "source": [
    "# initialize list of submissions\n",
    "month_submissions = []\n",
    "\n",
    "# run data request for \"submissions\" (original post)\n",
    "print('Downloading submissions for {}-{}'.format(year, month))\n",
    "hour_step = 6\n",
    "for hour in range(start_hour, end_hour, -hour_step):\n",
    "    url_params.update({'before': str(hour)+'h', 'after': str(hour+hour_step) + 'h'})\n",
    "    pushshift_response = json.loads(requests.get(submission_url, params=url_params).text)\n",
    "    month_submissions.extend(pushshift_response['data'])\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example JSON response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'manuginobilistan',\n",
       " 'author_flair_css_class': None,\n",
       " 'author_flair_text': None,\n",
       " 'brand_safe': True,\n",
       " 'can_mod_post': False,\n",
       " 'contest_mode': False,\n",
       " 'created_utc': 1514775573,\n",
       " 'domain': 'self.nba',\n",
       " 'full_link': 'https://www.reddit.com/r/nba/comments/7ncuar/get_manu_ginobili_to_be_a_west_captain/',\n",
       " 'id': '7ncuar',\n",
       " 'is_crosspostable': False,\n",
       " 'is_reddit_media_domain': False,\n",
       " 'is_self': True,\n",
       " 'is_video': False,\n",
       " 'locked': False,\n",
       " 'num_comments': 0,\n",
       " 'num_crossposts': 0,\n",
       " 'over_18': False,\n",
       " 'parent_whitelist_status': 'all_ads',\n",
       " 'permalink': '/r/nba/comments/7ncuar/get_manu_ginobili_to_be_a_west_captain/',\n",
       " 'pinned': False,\n",
       " 'retrieved_on': 1514848605,\n",
       " 'score': 1,\n",
       " 'selftext': '[removed]',\n",
       " 'spoiler': False,\n",
       " 'stickied': False,\n",
       " 'subreddit': 'nba',\n",
       " 'subreddit_id': 't5_2qo4s',\n",
       " 'subreddit_type': 'public',\n",
       " 'thumbnail': 'default',\n",
       " 'title': 'GET MANU GINOBILI TO BE A WEST CAPTAIN',\n",
       " 'url': 'https://www.reddit.com/r/nba/comments/7ncuar/get_manu_ginobili_to_be_a_west_captain/',\n",
       " 'whitelist_status': 'all_ads'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_submissions[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing the submissions\n",
    "Pushshift returns a JSON object for each query, which includes fields for the submission text, user, and more. To extract these, I created a helper function `parse_submission_pushshift`, which returns the values as a tuple. This is most useful for handling missing fields in the JSON gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Lebron and Kevin Durant enter a gym, who leaves first?. Both are known to have insane workouts and work ethic, both are trying to be the best in the world, so who leaves the gym first?',\n",
       " 1514762814,\n",
       " 'DaLaTy',\n",
       " 'Warriors',\n",
       " 0,\n",
       " '7nbrpc',\n",
       " '7nbrpc',\n",
       " '7nbrpc')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_submission_pushshift(month_submissions[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this parser function, I create a dataframe for the month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made dataframe of shape (22953, 9)\n"
     ]
    }
   ],
   "source": [
    "ops = [ parse_submission_pushshift(submission) for submission in month_submissions]\n",
    "data_col = ['text', 'timestamp', 'user', 'flair', 'score', 'id', 'link_id', 'parent_id']\n",
    "submission_df =(pd.DataFrame(ops, columns=data_col)\n",
    "                  .assign(source = lambda x: 'submission') )\n",
    "print('Made dataframe of shape {}'.format(submission_df.shape) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processed dataframe contains the following data:\n",
    "* text: the text from the submissions or comment. These can include emojis\n",
    "* timestamp: unix timestamp for the post\n",
    "* user: the reddit username of the post author\n",
    "* flair: subreddits allow users to express a \"flair,\" for example \"[CLE]\"\n",
    "* score: the net karma for the submissions or post\n",
    "* id: the unique ID for the post or submssion\n",
    "* link_id: this is the ID for the original post\n",
    "* parent_id: for comments, this is the id of the \"parent\" post in the thread; for submissions, this is the same as the link_id\n",
    "* source: whether the text is from a submission (original post) or comment (reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user</th>\n",
       "      <th>flair</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GAME THREAD: Memphis Grizzlies (11-25) @ Sacramento Kings (12-23) - (December 31, 2017). ##Gener...</td>\n",
       "      <td>1514762996</td>\n",
       "      <td>eatmorebread</td>\n",
       "      <td>Kings</td>\n",
       "      <td>6</td>\n",
       "      <td>7nbs8v</td>\n",
       "      <td>7nbs8v</td>\n",
       "      <td>7nbs8v</td>\n",
       "      <td>submission</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kyrie on what he'd do if he wasn't a NBA player. [deleted]</td>\n",
       "      <td>1514763030</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>7nbsdc</td>\n",
       "      <td>7nbsdc</td>\n",
       "      <td>7nbsdc</td>\n",
       "      <td>submission</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "0  GAME THREAD: Memphis Grizzlies (11-25) @ Sacramento Kings (12-23) - (December 31, 2017). ##Gener...   \n",
       "1                                           Kyrie on what he'd do if he wasn't a NBA player. [deleted]   \n",
       "\n",
       "    timestamp          user  flair  score      id link_id parent_id  \\\n",
       "0  1514762996  eatmorebread  Kings      6  7nbs8v  7nbs8v    7nbs8v   \n",
       "1  1514763030     [deleted]   None      1  7nbsdc  7nbsdc    7nbsdc   \n",
       "\n",
       "       source  \n",
       "0  submission  \n",
       "1  submission  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `get_month_pushshift` in `scrape_reddit_comments` repeats this process for the comments, using a smaller time step (one hour), and a different helper function (`parse_comment_pushshift`). For details, I would suggest looking at the [module](https://github.com/map222/trailofpapers/blob/sentiment_sports/sentiment_sports/scrape_reddit_comments.py).\n",
    "### Scraping data using the module\n",
    "Once you understand the basic code for scraping, you can do things more easily using the `get_month_pushshift` function. The function takes four parameters:\n",
    "* year: integer of year you want to scrape (I scraped back to ~2013)\n",
    "* month: integer of month you want to scrape\n",
    "* last_day: integer of last day of month; I manually changed this for every month; you should probably use a lookup table!\n",
    "* league: str name of subreddit you want to scrape. I used `nfl` and `nba`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "subreddit = 'nba'\n",
    "year = 2015\n",
    "month = 12\n",
    "last_day = 2\n",
    "month_df, submissions, comments = src.get_month_pushshift(year,month, last_day, subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above code usually takes 20-25 minutes for a full month (the example above only does 2 days). Pushshift typically yields ~300k comments / month for /r/NBA during the season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user</th>\n",
       "      <th>flair</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99440</th>\n",
       "      <td>MARK JACKSON FOR GM! #thenightmare</td>\n",
       "      <td>1.452452e+09</td>\n",
       "      <td>Asprobouboulis</td>\n",
       "      <td>Spurs</td>\n",
       "      <td>8.0</td>\n",
       "      <td>cyt3rc8</td>\n",
       "      <td>t3_40cist</td>\n",
       "      <td>t3_40cist</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91900</th>\n",
       "      <td>I don't think that anybody is going to try to argue that Lebron is a better scorer than MJ. Care...</td>\n",
       "      <td>1.452380e+09</td>\n",
       "      <td>BIGJ0N</td>\n",
       "      <td>Charlotte Hornets</td>\n",
       "      <td>5.0</td>\n",
       "      <td>cys8p4l</td>\n",
       "      <td>t3_40728l</td>\n",
       "      <td>t1_cys0qgl</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                      text  \\\n",
       "99440                                                                   MARK JACKSON FOR GM! #thenightmare   \n",
       "91900  I don't think that anybody is going to try to argue that Lebron is a better scorer than MJ. Care...   \n",
       "\n",
       "          timestamp            user              flair  score       id  \\\n",
       "99440  1.452452e+09  Asprobouboulis              Spurs    8.0  cyt3rc8   \n",
       "91900  1.452380e+09          BIGJ0N  Charlotte Hornets    5.0  cys8p4l   \n",
       "\n",
       "         link_id   parent_id   source  \n",
       "99440  t3_40cist   t3_40cist  comment  \n",
       "91900  t3_40728l  t1_cys0qgl  comment  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_df.sample(2, random_state=24607)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading NFL comments\n",
    "While I have focused here on downloading NBA comments, I repeated this task for the NFL as well. This was as simple as changing the subreddit to \"nfl\" and changing the dates to the appropriate months.\n",
    "### Save / load\n",
    "Finally, after downloading the comments, I save them to disk. Note the use of f-strings, which allow you to use the league, year, and month parameters to name your file. (Something weird is going on where this throws a `UnicodeDecodeError` when writing to a gzipped file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_df.to_csv(data_path + f'{league}_reddit_comments/{year}{month:02}-comments_submissions.tsv',\n",
    "                sep = '\\t', encoding = 'utf-8',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining covariate data\n",
    "To understand what drives player sentiment, I also needed to get demographic and performance data. This scraping was less interesting than scraping the reddit comments, so here is a sample of the covariate data for two players (details of the covariate scraping are in the appendix):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>season</th>\n",
       "      <th>Race</th>\n",
       "      <th>Ht</th>\n",
       "      <th>Wt</th>\n",
       "      <th>G</th>\n",
       "      <th>PER</th>\n",
       "      <th>PPG</th>\n",
       "      <th>TRBP</th>\n",
       "      <th>FTr</th>\n",
       "      <th>salary</th>\n",
       "      <th>experience</th>\n",
       "      <th>Wins</th>\n",
       "      <th>white_black_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>iman shumpert</td>\n",
       "      <td>2016</td>\n",
       "      <td>B</td>\n",
       "      <td>77.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.184</td>\n",
       "      <td>9662922.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>james ennis</td>\n",
       "      <td>2017</td>\n",
       "      <td>B</td>\n",
       "      <td>79.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>7.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.302</td>\n",
       "      <td>3028410.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Player  season Race    Ht     Wt     G   PER  PPG  TRBP    FTr  \\\n",
       "1101  iman shumpert    2016    B  77.0  220.0  76.0   9.0  7.5   6.2  0.184   \n",
       "1183    james ennis    2017    B  79.0  210.0  72.0  11.6  7.1   8.0  0.302   \n",
       "\n",
       "         salary  experience  Wins  white_black_diff  \n",
       "1101  9662922.0         5.0  51.0              54.0  \n",
       "1183  3028410.0         3.0  22.0               0.6  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_df = pd.read_csv('../modeling_data/nba_model_data.tsv',sep='\\t')\n",
    "example_df.sample(2, random_state=2016)\\\n",
    "           [['Player', 'season', 'Race', 'Ht', 'Wt', 'G', 'PER','PPG', 'TRBP', 'FTr', 'salary', 'experience', 'Wins', 'white_black_diff']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of some covariates in the sample:\n",
    "* Player: lowercase name of the player\n",
    "* Season: year the performance and age data comes from\n",
    "* Race: (W)hite, (B)lack, (A)sian\n",
    "* Ht / Wt: Height in inches and weight in pounds\n",
    "* G: Games played that season\n",
    "* PER: Player efficiency rating, an advanced stat characterizing overall offense\n",
    "* PPG: points per game\n",
    "* TRBP: total rebound percentage\n",
    "* FTr: Free throws attempted per shot\n",
    "* salary: dollar value of salary\n",
    "* experience: # of years in the league (min 1)\n",
    "* Wins: Team wins that year for team the player was on the most\n",
    "* white_black_diff: For a given city, the difference between the percent population that is white - percent of population that is black (white% - black%).\n",
    "\n",
    "There are a whole host of other performance columns not shown. The [full dataset is on github](https://github.com/map222/trailofpapers/blob/sentiment_sports/sentiment_sports/modeling_data/).\n",
    "\n",
    "## Conclusion\n",
    "In part I of the project, I showed you how to scrape comments from reddit for the NBA; and how to use a function to make that process easier. In part II, I will show you how to analyze these comments to calculate sentiment towards player."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix<a id='appendix'></a> \n",
    "### Details of covariate scraping\n",
    "To repeatably scrape covariate data, I built a [set of functions](https://github.com/map222/trailofpapers/blob/sentiment_sports/sentiment_sports/scrape_player_data.py) that queried [basketball-reference.com](https://basketball-reference.com), [pro-football-reference.com](https://pro-football-reference.com), [footballoutsiders.com](https://footballoutsiders.com/), and Wikipedia. In this section I won't go over the specific code for each dataset, but just show how to query the data.\n",
    "#### Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrape_player_data as spd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NBA\n",
    "#### Scrape player performance\n",
    "I wrote a helper function, `get_year_performance`, to scrape NBA player info from `basketball-reference.com`. I use this function for each year, then simply concatenate the results into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance2018_df = spd.get_year_performance_nba(2018).assign(season = lambda row: row['year'] -1)\n",
    "performance2017_df = spd.get_year_performance_nba(2017).assign(season = lambda row: row['year'] -1)\n",
    "performance2016_df = spd.get_year_performance_nba(2016).assign(season = lambda row: row['year'] -1)\n",
    "performance2015_df = spd.get_year_performance_nba(2015).assign(season = lambda row: row['year'] -1)\n",
    "performance2014_df = spd.get_year_performance_nba(2014).assign(season = lambda row: row['year'] -1)\n",
    "performance2013_df = spd.get_year_performance_nba(2013).assign(season = lambda row: row['year'] -1)\n",
    "performance_df = pd.concat([performance2013_df, performance2014_df, performance2015_df, performance2016_df, performance2017_df, performance2018_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remember to save your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df.to_csv('nba_performance.tsv', sep='\\t', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Player demographics\n",
    "It was difficult to find a list of NBA player ethnicities, so I just googled the pictures of a few hundred players. I downloaded height and weight information from basketball-reference.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df = pd.read_csv(data_path + 'covariates/nba_players2013.tsv', sep ='\\t', encoding = 'utf-8')\n",
    "demo_df['Player'] = demo_df['Player'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While parsing the heights, it sometimes gets saved as a date. Convert these dates to inches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df['Ht'] = demo_df['Ht'].str.split('-')\n",
    "height_dict = {'Jun':72, 'Jul':84, 'May':60}\n",
    "height_dict.update({str(x):x for x in range(13)})\n",
    "height_dict.update({'00':0})\n",
    "demo_df['Ht'] = demo_df['Ht'].apply(lambda row: height_dict[row[0]]*12 + height_dict[row[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Team info\n",
    "I downloaded team performance data from basketball-reference.com using `pandas.read_html`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2014, 2019):\n",
    "    team_year_df = pd.concat(pd.read_html(f'https://www.basketball-reference.com/leagues/NBA_{year}.html')[:2]).assign(year = year)\n",
    "    team_year_df.to_csv(data_path + f'covariates/nba_teams{year}.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then hand edit the 3-letter team names\n",
    "team_df = pd.concat([pd.read_csv(data_path + 'covariates/nba_teams2014.tsv', sep='\\t'),\n",
    "                     pd.read_csv(data_path + 'covariates/nba_teams2015.tsv', sep='\\t'),\n",
    "                     pd.read_csv(data_path + 'covariates/nba_teams2016.tsv', sep='\\t'),\n",
    "                     pd.read_csv(data_path + 'covariates/nba_teams2017.tsv', sep='\\t'),\n",
    "                     pd.read_csv(data_path + 'covariates/nba_teams2018.tsv', sep='\\t')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salary\n",
    "I downloaded salary information from [HoopsHype](https://hoopshype.com/salaries/) using `pandas.read_html`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "def get_nba_salary_year(season):\n",
    "    return (pd.read_html(f'https://hoopshype.com/salaries/players/{season}-{season+1}/')[0]\n",
    "              .rename(columns = {str(season) + '/' +  str(season+1)[2:]: 'salary'})\n",
    "              .assign(season = season))\n",
    "salary_df = pd.concat([get_nba_salary_year(season) for season in range(2010, 2019)])[['Player', 'salary', 'season']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The salary format was a little wonky, so I used `locale` to fix it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df['Player'] = salary_df['Player'].str.lower()\n",
    "import locale\n",
    "locale.setlocale( locale.LC_ALL, 'en_US.UTF-8' ) \n",
    "salary_df['salary'] = salary_df['salary'].str[1:].apply(locale.atoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salary standardization\n",
    "For regression, I also tried to standardize the salaries.  After taking the 4th root, things looked decent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df['standard_salary'] = np.power(salary_df['salary'], 1/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.power(salary_df['salary'], 1/4).hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City information\n",
    "When thinking about race, we considered whether the demographics of the city influence player popularity. I downloaded census and polling data for both NBA and NFL cities (I forgot the sites). I then spent some time making sure all the join keys were correct.\n",
    "#### City demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# city demographics\n",
    "city_df = pd.read_csv('d:/data/sentiment_sports/covariates/sports_metro_demographics.csv').drop(columns = ['census_Id', 'census_Id2', 'Geography'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### City voting during 2016 election"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vote_df = pd.read_excel('d:/data/sentiment_sports/covariates/2016 election results by county.xlsx')\n",
    "vote_df['county_name'] = vote_df['county_name'].str.lower().str.replace(' county', '')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "msa_df = pd.read_excel('data/covariates/County to MSA Map.xlsx')\n",
    "msa_df['County'] = msa_df['County'].str.lower()\n",
    "msa_df = msa_df.rename(columns = {'County':'county_name',\n",
    "                                  'State':'state_abbr'})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vote_df = vote_df.merge(msa_df, on= ['county_name', 'state_abbr']).drop_duplicates(['state_abbr', 'county_name'])\n",
    "vote_df.sort_values('total_votes', ascending=False).to_csv('data/covariates/msa_votes.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_df = pd.read_csv('d:/data/sentiment_sports/covariates/msa_votes.tsv', sep='\\t')[['per_point_diff', 'Tm']].rename(columns={'per_point_diff':'clinton_vote_lead'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine info\n",
    "After getting all of the data for various covariates, I combined them into a single dataframe that has performance, demographics, and city information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Race</th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Pos_x</th>\n",
       "      <th>Ht</th>\n",
       "      <th>Wt</th>\n",
       "      <th>Birth Date</th>\n",
       "      <th>Colleges</th>\n",
       "      <th>Rk</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>season</th>\n",
       "      <th>Wins</th>\n",
       "      <th>total_population</th>\n",
       "      <th>metro_percent_white</th>\n",
       "      <th>metro_percent_black</th>\n",
       "      <th>clinton_vote_lead</th>\n",
       "      <th>salary</th>\n",
       "      <th>standard_salary</th>\n",
       "      <th>experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alex abrines</td>\n",
       "      <td>W</td>\n",
       "      <td>2017</td>\n",
       "      <td>2018</td>\n",
       "      <td>G-F</td>\n",
       "      <td>78</td>\n",
       "      <td>190</td>\n",
       "      <td>August 1, 1993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>2016</td>\n",
       "      <td>47</td>\n",
       "      <td>1337075</td>\n",
       "      <td>74.1</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.1051</td>\n",
       "      <td>5994764.0</td>\n",
       "      <td>49.481519</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>steven adams</td>\n",
       "      <td>W</td>\n",
       "      <td>2014</td>\n",
       "      <td>2018</td>\n",
       "      <td>C</td>\n",
       "      <td>84</td>\n",
       "      <td>255</td>\n",
       "      <td>July 20, 1993</td>\n",
       "      <td>University of Pittsburgh</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>2016</td>\n",
       "      <td>47</td>\n",
       "      <td>1337075</td>\n",
       "      <td>74.1</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.1051</td>\n",
       "      <td>3140517.0</td>\n",
       "      <td>42.096917</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Player Race  From    To Pos_x  Ht   Wt      Birth Date  \\\n",
       "0  alex abrines    W  2017  2018   G-F  78  190  August 1, 1993   \n",
       "1  steven adams    W  2014  2018     C  84  255   July 20, 1993   \n",
       "\n",
       "                   Colleges  Rk     ...      year  season  Wins  \\\n",
       "0                       NaN   1     ...      2017    2016    47   \n",
       "1  University of Pittsburgh   3     ...      2017    2016    47   \n",
       "\n",
       "   total_population  metro_percent_white  metro_percent_black  \\\n",
       "0           1337075                 74.1                 10.2   \n",
       "1           1337075                 74.1                 10.2   \n",
       "\n",
       "   clinton_vote_lead     salary  standard_salary  experience  \n",
       "0             0.1051  5994764.0        49.481519           0  \n",
       "1             0.1051  3140517.0        42.096917           3  \n",
       "\n",
       "[2 rows x 69 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covariates_df = (demo_df.merge(performance_df, on = 'Player')\n",
    "                      .merge(team_df, on=['Tm', 'year'])\n",
    "                      .merge(city_df, on='Tm')\n",
    "                      .merge(vote_df, on='Tm') )\n",
    "covariates_df['Player'] = covariates_df['Player'].str.replace('.', '')\n",
    "covariates_df = covariates_df.merge(salary_df, on=['Player', 'season'], how='left')\n",
    "covariates_df['experience'] = covariates_df['year'] - covariates_df['From']\n",
    "covariates_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coach data\n",
    "In addition to getting player data, I wanted to get coach data. For coaches, we have fewer features (just wins). Here, I wrote a simple function to scrape basketball-reference.com, `scrape_nba_coaches`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_coach_df = spd.scrape_nba_coaches()\n",
    "nba_coach_df.to_csv('d:/data/sentiment_sports/covariates/nba_coach_performance.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# then hand edit it to assign race\n",
    "nba_coach_df.query('season >= 2013')[['Coach']].drop_duplicates().to_csv('d:/data/sentiment_sports/covariates/nba_coach_race.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NFL data\n",
    "In addition to understanding what drives NBA player sentiment, I wanted to look at other sports, specifically the NFL. Rather than step through each individual function to scrape the data, I would instead refer you to the `scrape_player_data` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PRAW\n",
    "Before using pushshift, I used the official reddit API via the [praw](https://praw.readthedocs.io/en/latest/) module. Unfortunately, an API update broke this code, so this is mostly here for posterity. If you figure out how to run this, let me know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import praw # for direct reddit pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client_id = 'nope'\n",
    "secret = 'nope'\n",
    "user_agent = 'r/nba race sentiment 0.1 by /u/Umiy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = praw.Reddit(user_agent=user_agent, client_id=client_id, client_secret=secret)\n",
    "r_nba = r.subreddit('nba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_submission( submission):\n",
    "    text = submission.title + '. ' + submission.selftext\n",
    "    creation_date = submission.created\n",
    "    author = submission.author #.name for PRAW\n",
    "    flair = submission.author_flair_text\n",
    "    score = submission.score\n",
    "    return (text, creation_date, author, flair, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_comment( comment):\n",
    "    if hasattr(comment, 'body') and comment.author != None:\n",
    "        text = comment.body\n",
    "        creation_date = comment.created\n",
    "        author = comment.author\n",
    "        flair = comment.author_flair_text\n",
    "        score = comment.score\n",
    "        return (text, creation_date, author, flair, score)\n",
    "    return ('', 1, '', '', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_month(year, month, month_length):\n",
    "    ''' Get comments and submissions for one month. takes ~ 1h 15 minutes.\n",
    "    '''\n",
    "    data_col = ['text', 'timestamp', 'user', 'flair']\n",
    "    print('Downloading submssions for {}-{}'.format(year, month))\n",
    "    month_submissions = [list(r_nba.submissions(datetime(year,month,day).timestamp(), datetime(year,month,day+1).timestamp() )) for day in range(1,month_length)]\n",
    "    month_submissions = [x for day_submissions in month_submissions for x in day_submissions]\n",
    "    \n",
    "    print('Downloaded {} submissions'.format(len(month_submissions)))\n",
    "    ops = [ parse_submission(submission) for submission in month_submissions]\n",
    "    submission_df =(pd.DataFrame(ops, columns=data_col)\n",
    "                      .assign(source = lambda x: 'submission') )\n",
    "    print('Made dataframe of shape {}'.format(submission_df.shape) )\n",
    "    \n",
    "    print('Downloading comments (this could take an hour)')\n",
    "    comments_list = [submission.comments.list() for submission in month_submissions if hasattr(submission, 'comments')]\n",
    "    comments = [ parse_comment(comment) for comments in  comments_list for comment in comments]\n",
    "    print('Downloaded {} comments'.format(len(comments) ) )\n",
    "    comment_df = (pd.DataFrame(comments, columns=data_col)\n",
    "                    .assign(source = lambda x: 'comment') )\n",
    "    return pd.concat([submission_df, comment_df]), ops, comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# this no longer works \n",
    "month_df, submissions, comments = get_month(2017, 11, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
