{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt at using spacy's NER training to do NER\n",
    "## This did not work well in the end. I labeled ~300 rows, and the recall on the labeled rows was not acceptable (labels with <5 examples were not identified well)\n",
    "### Things I improved / learned, despite the failure:\n",
    "* Found a better way to ensure dask uses all cpus: the `client()` method creates a local cluster that more consistently uses cores, and scales linearly with cores compared to the other methods (see `sports_sentiment.py`)\n",
    "* Learned how to train spacy's NER; learned that you really need a good number of examples for every known entity for it to get decent recall\n",
    "* improved sentence extraction by removing trailing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.curdir, '..', 'sentiment_sports'))\n",
    "import sports_sentiment as ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a list of all skins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "skin_df = pd.read_csv('labeled_skin_covariates.tsv', sep='\\t')\n",
    "skin_set = set(skin_df['skin_name'].str.lower().tolist()) | set(skin_df['skin_name'].tolist())\n",
    "# remove . from acronym skins\n",
    "skin_set = skin_set | set(skin.replace('.', '') for skin in skin_set )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training dataset for Spacy\n",
    "Load most recent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312235, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user</th>\n",
       "      <th>flair</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124159</th>\n",
       "      <td>How does someone this young have reddit???</td>\n",
       "      <td>1.562549e+09</td>\n",
       "      <td>ElevatedRaptor6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>et881zg</td>\n",
       "      <td>t3_caemjg</td>\n",
       "      <td>t3_caemjg</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113952</th>\n",
       "      <td>did they even have permission to do this?</td>\n",
       "      <td>1.562466e+09</td>\n",
       "      <td>Stealth_Reflex</td>\n",
       "      <td>:stealthreflex: Stealth Reflex</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>et57av7</td>\n",
       "      <td>t3_ca18hf</td>\n",
       "      <td>t3_ca18hf</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165746</th>\n",
       "      <td>Damn. Why it be like dis man</td>\n",
       "      <td>1.562882e+09</td>\n",
       "      <td>R34CTz</td>\n",
       "      <td>:oblivion: Oblivion</td>\n",
       "      <td>2.0</td>\n",
       "      <td>etjv6pd</td>\n",
       "      <td>t3_cbwgsi</td>\n",
       "      <td>t1_etjv02r</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text     timestamp  \\\n",
       "124159  How does someone this young have reddit???  1.562549e+09   \n",
       "113952   did they even have permission to do this?  1.562466e+09   \n",
       "165746                Damn. Why it be like dis man  1.562882e+09   \n",
       "\n",
       "                   user                           flair  score       id  \\\n",
       "124159  ElevatedRaptor6                             NaN    2.0  et881zg   \n",
       "113952   Stealth_Reflex  :stealthreflex: Stealth Reflex   -9.0  et57av7   \n",
       "165746           R34CTz             :oblivion: Oblivion    2.0  etjv6pd   \n",
       "\n",
       "          link_id   parent_id   source  \n",
       "124159  t3_caemjg   t3_caemjg  comment  \n",
       "113952  t3_ca18hf   t3_ca18hf  comment  \n",
       "165746  t3_cbwgsi  t1_etjv02r  comment  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "july_df = (pd.read_csv('d:/data/fortnite/201907-fortnitebr-comments_submissions.tsv', sep='\\t')\n",
    "              .dropna(subset=['text']))\n",
    "print(july_df.shape)\n",
    "july_df.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the few things that this work helped me do was improve that sentence chunking using dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking into sentences\n",
      "Reshaping and fixing whitespace / punctuation\n",
      "Chunked into 552840 sentences\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "july_sentences_df = ss.chunk_comments_sentences(july_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find sentences that contain an exact token match that we care about (this will fail on multi-token names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_search(row, token_set):\n",
    "    ''' find sentences that contain a token; this will fail for multi-token entities\n",
    "    '''\n",
    "    set_row = set(row.split(' '))\n",
    "    return [token for token in token_set if token in set_row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "july_sentences_df['contained_skin'] = july_sentences_df['sentences'].apply(lambda row: token_search(row, skin_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "skin_sentences_df = july_sentences_df[july_sentences_df['contained_skin'].str.len() >0][['sentences', 'contained_skin']]\n",
    "skin_sentences_df['skin_str'] = skin_sentences_df['contained_skin'].str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "    (\"Who is Chaka Khan?\", [(7, 17, \"PERSON\")]),\n",
    "    (\"I like London and Berlin.\", [(7, 13, \"LOC\"), (18, 24, \"LOC\")]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sentences = skin_sentences_df.drop_duplicates(subset =['skin_str'])\n",
    "unique_sentences[['sentences', 'skin_str']].to_csv('d:/data/fortnite/skin_ner_examples.txt', index=None, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_annotation(row):\n",
    "    ''' create spacy annotation for a row\n",
    "    '''\n",
    "    annotations = []\n",
    "    for skin in row['contained_skin']:\n",
    "        \n",
    "        span = list(re.search(skin, row['sentences']).span())\n",
    "        span += [skin]\n",
    "        annotations.append( tuple(span))\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\map22\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "unique_sentences['annotation'] = unique_sentences.apply(create_annotation, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sentences.to_csv('d:/data/fortnite/ner_training_examples.tsv', sep='\\t',\n",
    "                       index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Can we have same styles for dark vanguard and dark voyager like mission specialist and moonwalker'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow_skin_annotation(sentence, skin_set):\n",
    "    \n",
    "    annotations = []\n",
    "    for skin in skin_set:\n",
    "        search_result = re.search(skin, sentence)\n",
    "        if search_result:\n",
    "            span = list(search_result.span())\n",
    "            span += [skin]\n",
    "            annotations.append( tuple(span))\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7, 18, 'Recon Scout'), (13, 18, 'Scout')]\n"
     ]
    }
   ],
   "source": [
    "sample = \"Today, Recon Scout came back to the shop and it has not been in shoo since March 17 2018 (395 days)\"\n",
    "print(slow_skin_annotation(sample, skin_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Lynx' in skin_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of NER training using spacy\n",
    "### This did not yield good results; seems like I basically need to label each skin 5x to get it recognized \n",
    "Have labeled 200 rows for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_skins_df = pd.read_csv('ner_training_examples.tsv', sep='\\t', nrows=300)\n",
    "labeled_skins_df['annotation'] = labeled_skins_df['annotation'].fillna('[]')\n",
    "labeled_skins_df['annotation'] = labeled_skins_df['annotation'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_skin(annotation_list):\n",
    "    return [list(annotation[:2]) + ['SKIN'] for annotation in annotation_list]\n",
    "labeled_skins_df['annotation'] =labeled_skins_df['annotation'].apply(rename_skin)\n",
    "spacy_ner_train_data = list(labeled_skins_df[['sentences', 'annotation']].values)\n",
    "spacy_ner_train_data = [ (sentence, {'entities':entities}) for sentence, entities in spacy_ner_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ner = nlp.get_pipe('ner')\n",
    "ner.add_label('SKIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = nlp.resume_training()\n",
    "move_names = list(ner.move_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 3890.19620668759}\n",
      "Losses {'ner': 3536.268500545567}\n",
      "Losses {'ner': 3366.1052768380628}\n",
      "Losses {'ner': 3259.820517916717}\n",
      "Losses {'ner': 3196.235129074468}\n",
      "Losses {'ner': 3151.2381334613165}\n",
      "Losses {'ner': 2995.463249539025}\n",
      "Losses {'ner': 2987.788801067276}\n",
      "Losses {'ner': 2953.784096196294}\n",
      "Losses {'ner': 2928.1729186736047}\n",
      "Losses {'ner': 3045.6879339404404}\n",
      "Losses {'ner': 2955.5015856027603}\n",
      "Losses {'ner': 2850.174688756466}\n",
      "Losses {'ner': 2791.492754817009}\n",
      "Losses {'ner': 2950.26401770761}\n"
     ]
    }
   ],
   "source": [
    "n_iter = 15\n",
    "\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    sizes = compounding(1.0, 4.0, 1.001)\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    for itn in range(n_iter):\n",
    "        random.shuffle(spacy_ner_train_data)\n",
    "        batches = minibatch(spacy_ner_train_data, size=sizes)\n",
    "        losses = {}\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "        print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok I was smart and added a flare\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "test_sentence = july_sentences_df.sample(1).iloc[0]['sentences']\n",
    "test_sentence = spacy_ner_train_data[random.randint(0,199)][0]\n",
    "print(test_sentence)\n",
    "print(nlp(test_sentence).ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
